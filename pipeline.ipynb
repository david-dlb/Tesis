{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba36e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb\n",
    "# !pip install langchain huggingface-hub torch  # para embeddings si usas HuggingFace\n",
    "# !pip install -U langchain-community\n",
    "# !pip install spacy\n",
    "# !python -m spacy download es_core_news_sm\n",
    "# !pip install sentence_transformers\n",
    "# !pip install numpy\n",
    "# !pip install  sklearn\n",
    "# !pip install -U spacy sentence-transformers scikit-learn umap-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69be9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DB/1', 'r') as file:\n",
    "    data1 = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7fdd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/David/Programacion/CC/Tesis/Tesis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"hiiamsid/sentence_similarity_spanish_es\"  # Modelo en español\n",
    ")\n",
    "import spacy\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "# Configurar cliente Chroma (persistente en disco)\n",
    "if \"doc\" in [col.name for col in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(\"doc\")\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"doc\",\n",
    ")\n",
    "\n",
    "def chunk_text(text, strategy='fixed', size=100):\n",
    "  def hierarchical_chunk_and_store(doc_id=\"doc1\", section_size=5):\n",
    "    if \"hierarchical\" in [col.name for col in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(\"hierarchical\")\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=\"hierarchical\",\n",
    "    )\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    parent_ids = []\n",
    "\n",
    "    # Agrupar oraciones en secciones (padres)\n",
    "    for i in range(0, len(sentences), section_size):\n",
    "        parent_chunk = \" \".join(sentences[i:i+section_size])\n",
    "        parent_id = f\"{doc_id}_parent_{i//section_size}\"\n",
    "        parent_ids.append(parent_id)\n",
    "\n",
    "        # Guardar el chunk padre (opcional, útil para reconstrucción)\n",
    "        collection.add(\n",
    "            documents=[parent_chunk],\n",
    "            metadatas=[{\"doc_id\": doc_id, \"chunk_text\": parent_chunk, \"is_parent\": True}],\n",
    "            ids=[parent_id]\n",
    "        )\n",
    "\n",
    "        # Guardar hijos con referencia al padre\n",
    "        for j, sent in enumerate(sentences[i:i+section_size]):\n",
    "            chunk_id = f\"{doc_id}_chunk_{i+j}\"\n",
    "            chunks.append(sent)\n",
    "            ids.append(chunk_id)\n",
    "            metadatas.append({\"doc_id\": doc_id, \"chunk_text\": sent, \"parent_id\": parent_id, \"is_parent\": False})\n",
    "\n",
    "    # Guardar hijos\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    return ids, parent_ids\n",
    "\n",
    "  def paragraph(text, doc_id=\"doc1\"):\n",
    "    if \"paragraph\" in [col.name for col in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(\"paragraph\")\n",
    "    collection = chroma_client.create_collection(name=\"paragraph\")\n",
    "\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "\n",
    "    # Aquí guardamos directamente los textos (strings)\n",
    "    chunks = paragraphs\n",
    "    ids = [f\"{doc_id}_chunk{i}\" for i in range(len(chunks))]\n",
    "    metadatas = [{\"doc_id\": doc_id, \"chunk_text\": chunk} for chunk in chunks]\n",
    "\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    return chunks\n",
    "\n",
    "\n",
    "  def semantic_chunking(doc_id=\"doc1\", group_size=2, percentile_threshold=90, model_name='all-MiniLM-L6-v2'):\n",
    "      \"\"\"\n",
    "      sentences: List of sentences (strings)\n",
    "      group_size: Number of sentences per group for distance calculation\n",
    "      percentile_threshold: Percentile above which breakpoints are selected\n",
    "      model_name: SentenceTransformer model name\n",
    "      Returns: List of chunked texts\n",
    "      \"\"\"\n",
    "\n",
    "      if \"semantic\" in [col.name for col in chroma_client.list_collections()]:\n",
    "          chroma_client.delete_collection(\"semantic\")\n",
    "      collection = chroma_client.create_collection(\n",
    "          name=\"semantic\",\n",
    "      )\n",
    "      nlp = spacy.load(\"es_core_news_sm\")\n",
    "      sentences = [sent.text.strip() for sent in nlp(text).sents if sent.text.strip()]\n",
    "      # Step 1: Compute sentence embeddings\n",
    "      model = SentenceTransformer(model_name)\n",
    "      embeddings = model.encode(sentences, convert_to_numpy=True)\n",
    "\n",
    "      # Step 2: Form groups and compute average embeddings per group\n",
    "      groups = [\n",
    "          embeddings[i:i+group_size]\n",
    "          for i in range(0, len(sentences), group_size)\n",
    "      ]\n",
    "      group_embeddings = [np.mean(g, axis=0) for g in groups]\n",
    "\n",
    "      # Step 3: Compute semantic distances between consecutive groups\n",
    "      from sklearn.metrics.pairwise import cosine_distances\n",
    "      distances = [\n",
    "          cosine_distances([group_embeddings[i]], [group_embeddings[i+1]])[0][0]\n",
    "          for i in range(len(group_embeddings)-1)\n",
    "      ]\n",
    "\n",
    "      # Step 4: Identify breakpoints using percentile threshold\n",
    "      threshold = np.percentile(distances, percentile_threshold)\n",
    "      breakpoints = [\n",
    "          i for i, d in enumerate(distances) if d >= threshold\n",
    "      ]\n",
    "\n",
    "      # Step 5: Split the sentences at breakpoints\n",
    "      chunk_indices = [0]\n",
    "      for bp in breakpoints:\n",
    "          # Each breakpoint is at the end of a group\n",
    "          chunk_indices.append((bp+1)*group_size)\n",
    "      chunk_indices.append(len(sentences))\n",
    "\n",
    "      # Step 6: Form the chunks\n",
    "      chunks = [\n",
    "          ' '.join(sentences[chunk_indices[i]:chunk_indices[i+1]])\n",
    "          for i in range(len(chunk_indices)-1)\n",
    "          if chunk_indices[i] < chunk_indices[i+1]\n",
    "      ]\n",
    "      ids = [f\"{doc_id}_chunk{i}\" for i in range(len(chunks))]\n",
    "      metadatas = [{\"doc_id\": doc_id, \"chunk_text\": chunk} for chunk in chunks]\n",
    "\n",
    "      collection.add(\n",
    "          documents=chunks,\n",
    "          metadatas=metadatas,\n",
    "          ids=ids\n",
    "      )\n",
    "      return chunks\n",
    "\n",
    "  # Estrategia 2: Basado en oraciones\n",
    "  def sentence_chunk(text, size):\n",
    "      nlp = spacy.load(\"es_core_news_sm\")\n",
    "      doc = nlp(text)\n",
    "      return [sent.text for sent in doc.sents]\n",
    "\n",
    "  def sentence_chunk_and_store(text, doc_id=\"doc1\"):\n",
    "    if \"sentence\" in [col.name for col in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(\"sentence\")\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=\"sentence\",\n",
    "    )\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    doc = nlp(text)\n",
    "    chunks = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "    # Guardar cada chunk con metadatos que referencien el texto original\n",
    "    ids = [f\"{doc_id}_chunk{i}\" for i in range(len(chunks))]\n",
    "    metadatas = [{\"doc_id\": doc_id, \"chunk_text\": chunk} for chunk in chunks]\n",
    "\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    return chunks\n",
    "\n",
    "  # Selección de estrategia\n",
    "  if strategy == 'semantic':\n",
    "      return semantic_chunking()\n",
    "  elif strategy == 'sentence':\n",
    "      return sentence_chunk(text, size)\n",
    "  elif strategy == 'sentence_db':\n",
    "      return sentence_chunk_and_store(text)\n",
    "  elif strategy == 'paragraph':\n",
    "      return paragraph(text)\n",
    "  elif strategy == 'hierarchical':\n",
    "      return hierarchical_chunk_and_store()\n",
    "  else:\n",
    "      raise ValueError(f\"Estrategia no soportada: {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a72870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(text, strategy):\n",
    "  def quitar_parentesis(texto):\n",
    "      # Eliminar texto entre paréntesis\n",
    "      texto_sin_parentesis = re.sub(r'\\([^)]*\\)', '', texto)\n",
    "\n",
    "      # Opcional: eliminar aclaraciones entre guiones largos o comas (simplificado)\n",
    "      texto_sin_aclaraciones = re.sub(r'—[^—]*—', '', texto_sin_parentesis)\n",
    "      texto_sin_aclaraciones = re.sub(r',[^,]*,', '', texto_sin_aclaraciones)\n",
    "\n",
    "      # Limpiar espacios extra\n",
    "      texto_limpio = re.sub(r'\\s+', ' ', texto_sin_aclaraciones).strip()\n",
    "      return texto_limpio\n",
    "\n",
    "  def simple():\n",
    "    import spacy\n",
    "\n",
    "    # Carga modelo spaCy en español\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    # Eliminar stopwords y signos de puntuación\n",
    "    palabras_filtradas = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    # Reconstruir texto sin stopwords\n",
    "    texto_filtrado = \" \".join(palabras_filtradas)\n",
    "\n",
    "    # Eliminar frases repetidas simples (por ejemplo, oraciones duplicadas)\n",
    "    oraciones = list(dict.fromkeys(texto_filtrado.split('. ')))  # elimina duplicados manteniendo orden\n",
    "\n",
    "    # Reconstruir texto final\n",
    "    texto_final = '. '.join(oraciones)\n",
    "\n",
    "    return texto_final\n",
    "\n",
    "\n",
    "  if strategy == 'simple':\n",
    "      return simple()\n",
    "  elif strategy == 'sentence':\n",
    "    return text\n",
    "  elif strategy == 'recursive':\n",
    "    return text\n",
    "  elif strategy == 'semantic':\n",
    "    return text\n",
    "  else:\n",
    "      raise ValueError(f\"Estrategia no soportada: {strategy}\")\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc92386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai\n",
    "import google.generativeai as genai\n",
    "from mistralai import Mistral\n",
    "\n",
    "def construir_prompt_clasico(contexto, consulta):\n",
    "    prompt = f\"Contexto:\\n{contexto}\\n\\nPregunta:\\n{consulta}\\n\\nRespuesta:\"\n",
    "    return prompt\n",
    "def construir_prompt_chat(contexto, consulta):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente experto que responde preguntas usando el contexto proporcionado. No inventes información, no pongas texto adicional y sé conciso.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Contexto:\\n{contexto}\\n\\nPregunta:\\n{consulta}\"}\n",
    "    ]\n",
    "    return prompt\n",
    "def construir_prompt(contexto, consulta):\n",
    "    instrucciones = (\n",
    "        \"Eres un asistente experto que responde preguntas basándose únicamente en el contexto proporcionado.\\n\"\n",
    "        \"No inventes información, no pongas texto adicional y sé conciso.\\n\\n\"\n",
    "    )\n",
    "    prompt = f\"{instrucciones}Contexto:\\n{contexto}\\n\\nPregunta:\\n{consulta}\\n\\nRespuesta:\"\n",
    "    return prompt\n",
    "\n",
    "def build_prompt(context, consulta, strategy):\n",
    "  if strategy == 0:\n",
    "    return construir_prompt_chat(context, consulta)\n",
    "  prompt = None\n",
    "  if strategy == 1:\n",
    "    prompt = construir_prompt_clasico(context, consulta)\n",
    "  if strategy == 2:\n",
    "    prompt = construir_prompt(context, consulta)\n",
    "  # return [{\n",
    "  #  \"role\": \"user\",\n",
    "  #  \"content\": prompt,\n",
    "  # }]\n",
    "  return prompt\n",
    "\n",
    "def generate(context, query, strategy):\n",
    "  client = genai.Client(api_key=\"AIzaSyCdcvLoAPS5uJyVI9w_yyer4OPM544vW9o\")\n",
    "  response = client.models.generate_content(\n",
    "      model=\"gemini-2.0-flash-lite\",\n",
    "      contents=build_prompt(context, query, strategy)\n",
    "  )\n",
    "  return response.text\n",
    "\n",
    "def send_menssage(mesg):\n",
    "  api_key = \"hiYKVGzE9NhITN7JRXeaBZMsaujppzPt\"\n",
    "  model = \"mistral-large-latest\"\n",
    "\n",
    "  client = Mistral(api_key=api_key)\n",
    "\n",
    "  chat_response = client.chat.complete(\n",
    "      model= model,\n",
    "      messages=mesg\n",
    "  )\n",
    "  return chat_response.choices[0].message.content\n",
    "\n",
    "  # response = client.chat.completions.create(\n",
    "  #     model=\"accounts/fireworks/models/llama-v3p3-70b-instruct\",\n",
    "  #     messages= build_prompt(context, query, strategy)\n",
    "  #   )\n",
    "  # return response.choices[0].message.content\n",
    "\n",
    "# client = Fireworks(api_key=\"fw_3ZbjyhcrMzAXRDTWRouf7SKo\")\n",
    "\n",
    "\n",
    "# generate(\"la inteligencia artificial se usa en entornos cerrrados\", \"donde se usa la inteligencia artificial\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7f7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_merging_retrieval(collection, query_text, n_results=10, merge_threshold=0.5):\n",
    "    # Recuperar los chunks más relevantes (hijos)\n",
    "    resultados = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results,\n",
    "        include=[\"metadatas\", \"documents\"]  # quitar \"ids\" de aquí\n",
    "    )\n",
    "    metadatas = resultados[\"metadatas\"][0]\n",
    "    documents = resultados[\"documents\"][0]\n",
    "    ids = resultados[\"ids\"][0]  # ids vienen en el resultado, no en include\n",
    "\n",
    "    # Agrupar por parent_id\n",
    "    from collections import defaultdict\n",
    "    parent_to_children = defaultdict(list)\n",
    "    for idx, meta in enumerate(metadatas):\n",
    "        parent_id = meta.get(\"parent_id\")\n",
    "        if parent_id:\n",
    "            parent_to_children[parent_id].append((ids[idx], documents[idx]))\n",
    "\n",
    "    merged_docs = []\n",
    "    merged_ids = set()\n",
    "\n",
    "    for parent_id, children in parent_to_children.items():\n",
    "        # Obtener total hijos de ese padre en la BD\n",
    "        all_metas = collection.get(include=[\"metadatas\"])[\"metadatas\"]\n",
    "        total_hijos = sum(1 for m in all_metas if m.get(\"parent_id\") == parent_id)\n",
    "\n",
    "        if len(children) / max(1, total_hijos) >= merge_threshold:\n",
    "            # Recuperar el chunk padre\n",
    "            parent_doc = collection.get(ids=[parent_id])[\"documents\"][0]\n",
    "            merged_docs.append(parent_doc)\n",
    "            merged_ids.add(parent_id)\n",
    "        else:\n",
    "            for cid, doc in children:\n",
    "                if cid not in merged_ids:\n",
    "                    merged_docs.append(doc)\n",
    "                    merged_ids.add(cid)\n",
    "\n",
    "    # También incluir chunks sin parent_id si quieres\n",
    "    # (opcional, según tu caso)\n",
    "\n",
    "    return merged_docs\n",
    "contextos = auto_merging_retrieval(collection, \"base de datos para textos en español\", n_results=10, merge_threshold=0.5)\n",
    "for c in contextos:\n",
    "    print(\"----\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6868f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "def kmeans_build(collection, query_text, n_results=10):\n",
    "    # Obtener todos los embeddings almacenados\n",
    "    items = collection.get(include=[\"embeddings\", \"metadatas\", \"documents\"])\n",
    "    embeddings = np.array(items[\"embeddings\"])  # Shape: (n_chunks, dim_embedding)\n",
    "\n",
    "    # Aplicar K-Means (ej: 5 clusters)\n",
    "    n_clusters = 5\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # Actualizar metadatos con la etiqueta de cluster\n",
    "    for idx, (id_, metadata) in enumerate(zip(items[\"ids\"], items[\"metadatas\"])):\n",
    "        metadata[\"cluster\"] = int(clusters[idx])\n",
    "        collection.update(ids=[id_], metadatas=[metadata])\n",
    "\n",
    "def get_relevant_clusters_and_texts(collection, query_text, n_results=10):\n",
    "    # Realizar consulta para obtener chunks relevantes\n",
    "    resultados = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results,\n",
    "        include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    metadatas = resultados[\"metadatas\"][0]   # lista de metadatos\n",
    "    documents = resultados[\"documents\"][0]   # lista de textos/chunks\n",
    "    distances = resultados[\"distances\"][0]   # lista de distancias\n",
    "\n",
    "    # Extraer clusters y agrupar textos por cluster\n",
    "    cluster_to_texts = defaultdict(list)\n",
    "    cluster_to_similarity = defaultdict(float)\n",
    "\n",
    "    for meta, doc, dist in zip(metadatas, documents, distances):\n",
    "        cluster = meta.get(\"cluster\", \"unknown\")\n",
    "        similarity = 1 - dist  # si distancia es coseno, similitud = 1 - distancia\n",
    "        cluster_to_texts[cluster].append(doc)\n",
    "        cluster_to_similarity[cluster] += similarity\n",
    "\n",
    "    # Contar frecuencia de clusters\n",
    "    cluster_counts = {cluster: len(texts) for cluster, texts in cluster_to_texts.items()}\n",
    "\n",
    "    # Ordenar clusters por frecuencia o similitud acumulada\n",
    "    clusters_ordenados = sorted(cluster_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Preparar resultado: lista de (cluster, frecuencia, similitud, textos)\n",
    "    resultado = []\n",
    "    for cluster, count in clusters_ordenados:\n",
    "        resultado.append({\n",
    "            \"cluster\": cluster,\n",
    "            \"frequency\": count,\n",
    "            \"total_similarity\": cluster_to_similarity[cluster],\n",
    "            \"texts\": cluster_to_texts[cluster]\n",
    "        })\n",
    "\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "198d9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En la estancia se desarrollaron cultivos aborígenes como la yuca y el tabaco, además de cultivos europeos, como hortalizas y granos, se emplearon técnicas agrícolas aborígenes y españolas.', 'Es el resultado de una mezcla\\nde etnias y culturas en constante proceso de transformación.', 'En la estancia convivían aborígenes, africanos y españoles, los dos primeros como fuerza de trabajo y el último como usufructuario de la tierra y dueño de todo lo que se producía.', 'Se interrumpió violentamente la vida de estos grupos aborígenes.', 'Ya conocemos el genocidio de la población aborigen durante la conquista y colonización.', 'Los aborígenes utilizaron por lo general el método de rebeldía que afectaba\\nmás a los colonialistas españoles: abandonaban el trabajo y huían a los montes,\\na zonas marginales de difícil acceso para evadir la explotación que sufrían, pero\\nFig.', '¿Cuáles son algunos de los procesos étnicos que se desarrollaron?7\\n• La asimilación étnica entre el componente hispano y el aborigen.', 'Existen evidencias materiales de “palenques” donde se refugiaban los aborígenes\\nque lograban escapar, para continuar su vida, coincidiendo con esclavos africanos\\nque, a pesar de ser muy escasos en estos primeros años de colonización, también se\\n“cimarroneaban” junto a los aborígenes para evadir la explotación.', 'El componente aborigen\\nAntes de la conquista española Cuba estaba habitada por diversas comunidades aborígenes.']\n"
     ]
    }
   ],
   "source": [
    "chunk_text(data1, strategy='sentence_db', size=50)\n",
    "chunk_text(data1, strategy='hierarchical', size=50)\n",
    "# chunk_text(data1, strategy='semantic', size=200)\n",
    "chunk_text(data1, strategy='paragraph', size=200)\n",
    "\n",
    "query = \"A que se dedicaban los aborigenes\"\n",
    "\n",
    "collection1 = chroma_client.get_collection(\"hierarchical\")\n",
    "print(auto_merging_retrieval(collection1, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f147c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.get_collection(\"paragraph\")\n",
    "def sentence_window_retrieval(collection, query, window_size=2):\n",
    "    # 1. Obtener todos los chunks y sus IDs en orden\n",
    "    data = collection.get( )\n",
    "    all_chunks = data[\"documents\"]  # lista de textos\n",
    "    all_ids = data[\"ids\"]           # lista de IDs, en el mismo orden que all_chunks\n",
    " \n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=2,\n",
    "    )\n",
    "\n",
    "    relevant_ids = results['ids'][0]  # Ejemplo: ['doc1_chunk5', 'doc1_chunk8']\n",
    "    # 3. Función para obtener ventana de tamaño 2 alrededor de un índice\n",
    "    def get_window_chunks_no_duplicates(all_chunks, all_ids, target_id, seen_chunks=set()):\n",
    "        try:\n",
    "            idx = all_ids.index(target_id)\n",
    "        except ValueError:\n",
    "            return \"\", seen_chunks  # ID no encontrado\n",
    "        \n",
    "        start = max(0, idx - window_size)\n",
    "        end = min(len(all_chunks), idx + window_size + 1)\n",
    "        \n",
    "        window_chunks = []\n",
    "        for i in range(start, end):\n",
    "            if i not in seen_chunks:\n",
    "                window_chunks.append(all_chunks[i])\n",
    "                seen_chunks.add(i)\n",
    "        \n",
    "        return \" \".join(window_chunks), seen_chunks\n",
    "\n",
    "    # Uso\n",
    "\n",
    "    seen = set()\n",
    "    expanded_contexts = []\n",
    "\n",
    "    for chunk_id in relevant_ids:\n",
    "        context, seen = get_window_chunks_no_duplicates(all_chunks, all_ids, chunk_id, seen_chunks=seen)\n",
    "        if context:\n",
    "            expanded_contexts.append(context)\n",
    "\n",
    "    result = \"\"\n",
    "    # Mostrar resultados\n",
    "    # for i, context in enumerate(expanded_contexts):\n",
    "    #     print(f\"Contexto expandido sin duplicados para {relevant_ids[i]}:\\n{context}\\n\")\n",
    "    #     result = result.join(context)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c55c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_merge(collection, query):\n",
    "    results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=2,\n",
    "        )\n",
    "    context = \"\"\n",
    "    for i in results['documents']:\n",
    "        for j in i:\n",
    "            context += j\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_historial(chunks, doc_id=\"doc1\"):\n",
    "    if \"historial\" in [col.name for col in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(\"historial\")\n",
    "    collection = chroma_client.create_collection(name=\"historial\")\n",
    " \n",
    "    ids = [f\"{doc_id}_chunk{i}\" for i in range(len(chunks))]\n",
    "    metadatas = [{\"doc_id\": doc_id, \"chunk_text\": chunk} for chunk in chunks]\n",
    "\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    return chunks\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d868a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m s_query = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mprompt\u001b[49m)\n\u001b[32m      7\u001b[39m     prompt = \u001b[33m\"\u001b[39m\u001b[33minput()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     prompt = \u001b[33m\"\u001b[39m\u001b[33mA que se dedicaban los aborigenes cubanos\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "s_context_historial = 0\n",
    "s_merge = 3\n",
    "s_query = 0\n",
    "s_historial_mode = 0\n",
    "\n",
    "historial = []\n",
    "\n",
    "for i in range(1):\n",
    "    print(prompt)\n",
    "    prompt = \"input()\"\n",
    "    prompt = \"A que se dedicaban los aborigenes cubanos\"\n",
    "    # optimizar prompt\n",
    "    prompt_o = compress(prompt, \"simple\")\n",
    "\n",
    "    if s_query == 0:\n",
    "        query = prompt\n",
    "    else: \n",
    "        query = prompt_o\n",
    "\n",
    "    if s_context_historial == 0:\n",
    "        # obtener contexto del documento\n",
    "        if s_merge == 0:\n",
    "            collection = chroma_client.get_collection(\"semantic\")\n",
    "            context = simple_merge(collection, query)   \n",
    "        if s_merge == 1:\n",
    "            collection = chroma_client.get_collection(\"paragraph\")\n",
    "            res = get_relevant_clusters_and_texts(collection, query) \n",
    "            context = \"\"\n",
    "            for i in res:\n",
    "                for j in i[\"texts\"]:\n",
    "                    context = context + j\n",
    "        if s_merge == 2:\n",
    "            collection = chroma_client.get_collection(\"sentence\")\n",
    "            context = sentence_window_retrieval(collection, query)   \n",
    "        if s_merge == 3:\n",
    "            collection = chroma_client.get_collection(\"hierarchical\")\n",
    "            res = auto_merging_retrieval(collection, query)\n",
    "            context = \"\"\n",
    "            for i in res:\n",
    "                context += i\n",
    "\n",
    "        # obtener contexto historial\n",
    "        store_historial(historial)\n",
    "        collection1 = chroma_client.get_collection(\"hierarchical\")\n",
    "        auto_merging_retrieval(collection1, query)\n",
    "\n",
    "        # unir historial y documento\n",
    "    else:\n",
    "        # collection1 = chroma_client.get_collection(\"hierarchical_historial\")\n",
    "        # print(auto_merging_retrieval(collection1, query))\n",
    "        context = \"\"\n",
    "\n",
    "\n",
    "    # print(\"context\", context)\n",
    "    # print(query)\n",
    "    context = compress(context, \"simple\")\n",
    "    msg = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": context + \" \" + query,\n",
    "        }\n",
    "    ]\n",
    "    response = send_menssage(msg)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e21ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer TU_API_KEY\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"¿Cuál es la capital de Perú?\"}]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd8a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing the \"best\" French cheese can be subjective and depends on personal taste, as France offers a wide variety of exceptional cheeses. However, some French cheeses are world-renowned for their unique flavors and qualities. Here are a few notable ones:\n",
      "\n",
      "1. **Camembert de Normandie**: A soft, creamy cheese from the Normandy region, famous for its rich, buttery flavor and bloomy rind.\n",
      "\n",
      "2. **Brie de Meaux**: Often referred to as the \"King of Cheeses,\" this soft cheese from the Brie region has a smooth, velvety texture and a mild, slightly nutty flavor.\n",
      "\n",
      "3. **Roquefort**: A classic blue cheese made from sheep's milk, known for its strong, tangy flavor and distinctive veins of blue mold.\n",
      "\n",
      "4. **Comté**: A hard cheese from the Jura region, known for its complex, nutty flavor and firm, slightly granular texture.\n",
      "\n",
      "5. **Reblochon**: A soft washed-rind and smear-ripened cheese from the Alps, often used in the traditional dish Tartiflette.\n",
      "\n",
      "6. **Époisses**: A strong, pungent cheese from Burgundy, known for its intense aroma and rich, creamy flavor.\n",
      "\n",
      "7. **Mimolette**: A hard cheese with a bright orange color, known for its fruity and slightly nutty flavor.\n",
      "\n",
      "Each of these cheeses has its own unique characteristics and flavors, so the \"best\" one will depend on your personal preferences.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key = \"hiYKVGzE9NhITN7JRXeaBZMsaujppzPt\"\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model= model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the best French cheese?\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf456744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== TextRank Summary ==\n",
      "- La compresión de contexto busca reducir el número de tokens sin perder información clave. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytextrank\n",
    "\n",
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# Cargar el modelo de lenguaje de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Añadir el pipeline de PyTextRank\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"\"\"\n",
    "En el presente trabajo se estudia el impacto de la compresión en modelos de lenguaje. \n",
    "Además, se revisa el estado del arte en técnicas extractivas. \n",
    "La compresión de contexto busca reducir el número de tokens sin perder información clave. \n",
    "El pipeline propuesto aplica técnicas como segmentación semántica y agrupamiento.\n",
    "Los resultados muestran mejoras del 23% en eficiencia de respuesta.\n",
    "\"\"\"\n",
    "\n",
    "# Procesar el texto\n",
    "doc = nlp(text)\n",
    "total_frases = len(list(doc.sents))\n",
    "limit_sentences = max(1, int(total_frases * 0.15))\n",
    "# Mostrar las 3 frases más importantes según TextRank\n",
    "print(\"== TextRank Summary ==\")\n",
    "for sent in doc._.textrank.summary(limit_phrases=15, limit_sentences=limit_sentences):\n",
    "    print(\"-\", sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bf859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Compresión automática ==\n",
      "En el presente trabajo se estudia el impacto de la compresión en modelos de lenguaje. Además, se revisa el estado del arte en técnicas extractivas. La compresión de contexto busca reducir el número de tokens sin perder información clave. Los resultados muestran mejoras del 23% en eficiencia de respuesta.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Función para evaluar automáticamente si una oración es valiosa\n",
    "def frase_valiosa_auto(oracion):\n",
    "    doc = nlp(oracion)\n",
    "\n",
    "    # Regla 1: descartar frases muy cortas o sin verbo\n",
    "    if len(oracion.split()) < 8 or not any(t.pos_ == \"VERB\" for t in doc):\n",
    "        return False\n",
    "\n",
    "    # Regla 2: conservar frases con entidades nombradas (números, organizaciones, etc.)\n",
    "    if any(ent.label_ in [\"PER\", \"ORG\", \"LOC\", \"MISC\", \"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"CARDINAL\", \"ORDINAL\", \"QUANTITY\"] for ent in doc.ents):\n",
    "        return True\n",
    "\n",
    "    # Regla 3: mantener si tiene estructura completa (sujeto + verbo + complemento)\n",
    "    tokens = [t.dep_ for t in doc]\n",
    "    estructura_completa = \"nsubj\" in tokens and \"ROOT\" in tokens and (\"obj\" in tokens or \"obl\" in tokens)\n",
    "    if estructura_completa:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"\"\"\n",
    "En el presente trabajo se estudia el impacto de la compresión en modelos de lenguaje. \n",
    "Además, se revisa el estado del arte en técnicas extractivas. \n",
    "La compresión de contexto busca reducir el número de tokens sin perder información clave. \n",
    "El pipeline propuesto aplica técnicas como segmentación semántica y agrupamiento.\n",
    "Los resultados muestran mejoras del 23% en eficiencia de respuesta.\n",
    "\"\"\"\n",
    "\n",
    "# Aplicar\n",
    "doc = nlp(texto)\n",
    "oraciones = [str(sent).strip() for sent in doc.sents]\n",
    "oraciones_filtradas = [o for o in oraciones if frase_valiosa_auto(o)]\n",
    "\n",
    "# Resultado\n",
    "texto_comprimido = \" \".join(oraciones_filtradas)\n",
    "print(\"== Compresión automática ==\")\n",
    "print(texto_comprimido)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42db97ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La primera pregunta que me hiciste fue: \"¿Cuál es la capital de Cuba?\"\n"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "import os\n",
    "\n",
    "client = Mistral(api_key=\"hiYKVGzE9NhITN7JRXeaBZMsaujppzPt\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuál es la capital de Cuba?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Eres un asistente útil.\"},\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuál la primera pregunta que te hize?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.complete(\n",
    "    model=\"mistral-large-latest\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
